{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4755c01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_0345_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_0346_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_0347_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_0348_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_0349_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  label\n",
       "0  IMG_0345_cropped.jpg      0\n",
       "1  IMG_0346_cropped.jpg      0\n",
       "2  IMG_0347_cropped.jpg      0\n",
       "3  IMG_0348_cropped.jpg      0\n",
       "4  IMG_0349_cropped.jpg      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('image_labels.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9496dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('image_labels.csv')\n",
    "df = df_train.sample(frac=1, random_state=123) \n",
    "\n",
    "train_ratio = 0.7  # 70% for training\n",
    "val_ratio = 0.15   # 15% for validation\n",
    "test_ratio = 0.15  # 15% for testing\n",
    "\n",
    "total_size = df.shape[0]\n",
    "train_end = round(total_size * train_ratio) \n",
    "val_end = round(total_size * (train_ratio + val_ratio))\n",
    "\n",
    "#create the splits at defined boundaries \n",
    "df_train = df.iloc[:train_end] \n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "df_train.to_csv('images_train.csv', index=False)\n",
    "df_val.to_csv('images_val.csv', index=False)\n",
    "df_test.to_csv('images_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0dfa0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 43\n",
      "Train: 30 (69.8%)\n",
      "Val: 7 (16.3%)\n",
      "Test: 6 (14.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total samples: {total_size}\")\n",
    "print(f\"Train: {len(df_train)} ({len(df_train)/total_size:.1%})\")\n",
    "print(f\"Val: {len(df_val)} ({len(df_val)/total_size:.1%})\")\n",
    "print(f\"Test: {len(df_test)} ({len(df_test)/total_size:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a534e461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_0345_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IMG_0370_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>IMG_0595_cropped.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IMG_0355_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>IMG_0412_cropped.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename  label\n",
       "0   IMG_0345_cropped.jpg      0\n",
       "14  IMG_0370_cropped.jpg      0\n",
       "40  IMG_0595_cropped.jpg      1\n",
       "9   IMG_0355_cropped.jpg      0\n",
       "32  IMG_0412_cropped.jpg      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15c094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a2866f0",
   "metadata": {},
   "source": [
    "Creating a Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a78a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms \n",
    "import pandas as pd \n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, hdf5_file, transform=None): \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.hdf5_file = hdf5_file                           \n",
    "        self.transform = transform\n",
    "\n",
    "        with h5py.File(hdf5_file,'r') as f:\n",
    "            self.dataset_keys = list(f.keys())\n",
    "            print(f\"HDf5 keys: {self.dataset_keys}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.data.iloc[idx, 0]\n",
    "        label = torch.tensor(self.data.iloc[idx, 1], dtype=torch.long)\n",
    "    \n",
    "        original_idx = self.data.index[idx]\n",
    "        hdf5_key = f'image_{original_idx}'\n",
    "        \n",
    "        with h5py.File(self.hdf5_file, 'r') as f:\n",
    "            if hdf5_key in f:\n",
    "                image_data = f[hdf5_key][:]\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot find HDF5 key '{hdf5_key}' for filename {filename}\")\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        image = Image.fromarray(image_data.astype('uint8'))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f5ea0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDf5 keys: ['image_0', 'image_1', 'image_10', 'image_11', 'image_12', 'image_13', 'image_14', 'image_15', 'image_16', 'image_17', 'image_18', 'image_19', 'image_2', 'image_20', 'image_21', 'image_22', 'image_23', 'image_24', 'image_25', 'image_26', 'image_27', 'image_28', 'image_29', 'image_3', 'image_30', 'image_31', 'image_32', 'image_33', 'image_34', 'image_35', 'image_36', 'image_37', 'image_38', 'image_39', 'image_4', 'image_40', 'image_41', 'image_42', 'image_43', 'image_44', 'image_45', 'image_46', 'image_47', 'image_48', 'image_49', 'image_5', 'image_50', 'image_51', 'image_52', 'image_53', 'image_54', 'image_55', 'image_56', 'image_57', 'image_58', 'image_59', 'image_6', 'image_60', 'image_61', 'image_62', 'image_63', 'image_64', 'image_65', 'image_66', 'image_67', 'image_68', 'image_7', 'image_8', 'image_9']\n",
      "HDf5 keys: ['image_0', 'image_1', 'image_10', 'image_11', 'image_12', 'image_13', 'image_14', 'image_15', 'image_16', 'image_17', 'image_18', 'image_19', 'image_2', 'image_20', 'image_21', 'image_22', 'image_23', 'image_24', 'image_25', 'image_26', 'image_27', 'image_28', 'image_29', 'image_3', 'image_30', 'image_31', 'image_32', 'image_33', 'image_34', 'image_35', 'image_36', 'image_37', 'image_38', 'image_39', 'image_4', 'image_40', 'image_41', 'image_42', 'image_43', 'image_44', 'image_45', 'image_46', 'image_47', 'image_48', 'image_49', 'image_5', 'image_50', 'image_51', 'image_52', 'image_53', 'image_54', 'image_55', 'image_56', 'image_57', 'image_58', 'image_59', 'image_6', 'image_60', 'image_61', 'image_62', 'image_63', 'image_64', 'image_65', 'image_66', 'image_67', 'image_68', 'image_7', 'image_8', 'image_9']\n",
      "HDf5 keys: ['image_0', 'image_1', 'image_10', 'image_11', 'image_12', 'image_13', 'image_14', 'image_15', 'image_16', 'image_17', 'image_18', 'image_19', 'image_2', 'image_20', 'image_21', 'image_22', 'image_23', 'image_24', 'image_25', 'image_26', 'image_27', 'image_28', 'image_29', 'image_3', 'image_30', 'image_31', 'image_32', 'image_33', 'image_34', 'image_35', 'image_36', 'image_37', 'image_38', 'image_39', 'image_4', 'image_40', 'image_41', 'image_42', 'image_43', 'image_44', 'image_45', 'image_46', 'image_47', 'image_48', 'image_49', 'image_5', 'image_50', 'image_51', 'image_52', 'image_53', 'image_54', 'image_55', 'image_56', 'image_57', 'image_58', 'image_59', 'image_6', 'image_60', 'image_61', 'image_62', 'image_63', 'image_64', 'image_65', 'image_66', 'image_67', 'image_68', 'image_7', 'image_8', 'image_9']\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std = [0.229, 0.224, 0.225]) # what are the numbers again\n",
    "])\n",
    "\n",
    "#now you create the data\n",
    "train_dataset = ImageDataset('images_train.csv', 'image_dataset.h5',transform = transform)\n",
    "val_dataset = ImageDataset('images_val.csv', 'image_dataset.h5',transform = transform)\n",
    "test_dataset = ImageDataset('images_test.csv', 'image_dataset.h5',transform = transform)\n",
    "\n",
    "# then call the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle= True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 4, shuffle= True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 4, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1) #tiny CNN\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        # For 224x224 input: after 2 pooling layers = 56x56 but hmm what does this do\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  #Binary classification as high or low\n",
    "        self.dropout = nn.Dropout(0.2) #to help out with learning best features \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "752228ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImageClassifier(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=100352, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "model = SimpleImageClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf909ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(modek, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels, = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() #initialize gradient, don't carry from previous \n",
    "        outputs = model(images) #outputs are values from the model \n",
    "        loss = criterion(outputs, labels) #compare values\n",
    "        loss.backward() #back prop to update weights\n",
    "        optimizer.step() #update weights with the optimizer!!\n",
    "\n",
    "        running_loss += loss.item() \n",
    "        _, predicted = torch.max(outputs.data, 1) #get the predictions from model \n",
    "        total += labels.size(0) #get the toal \n",
    "        correct += (predicted == labels).sum().item() #add to the correct\n",
    "\n",
    "    return running_loss /len(train_loader), 100 * correct / total #calculate accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321db939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels, = images.to(device), labels.to(device)\n",
    "            outputs = model(images) #no optimization here \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return val_loss / len+(val_loader), 100 * correct / total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90375964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5\n",
      "  Train Loss: 1.8111, Train Acc: 63.33%\n",
      "  Val Loss: 1.1182, Val Acc: 14.29%\n",
      "--------------------------------------------------\n",
      "2 5\n",
      "  Train Loss: 0.7853, Train Acc: 53.33%\n",
      "  Val Loss: 0.4805, Val Acc: 85.71%\n",
      "--------------------------------------------------\n",
      "3 5\n",
      "  Train Loss: 0.4149, Train Acc: 76.67%\n",
      "  Val Loss: 0.4963, Val Acc: 85.71%\n",
      "--------------------------------------------------\n",
      "4 5\n",
      "  Train Loss: 0.3554, Train Acc: 76.67%\n",
      "  Val Loss: 0.5367, Val Acc: 85.71%\n",
      "--------------------------------------------------\n",
      "5 5\n",
      "  Train Loss: 0.2727, Train Acc: 90.00%\n",
      "  Val Loss: 0.6317, Val Acc: 85.71%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(epoch+1, num_epochs)\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53460347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7b778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
